{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled9.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPNKNOrWPK659w5j40gFI3R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tanakakao/test/blob/main/transformer_torch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tjQ9CQ8rpJ6w"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optimizers\n",
        "\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    '''\n",
        "    Multi-Head Attentionレイヤ\n",
        "    \n",
        "    hidden_dim : Embeddingされた単語ベクトルの長さ\n",
        "    heads_num : マルチヘッドAttentionのヘッド数\n",
        "       ※hidden_numはheads_numで割り切れえる値とすること\n",
        "    drop_rate : 出力のDropout率\n",
        "\n",
        "    model = MultiheadAttention(\n",
        "        hidden_dim = 512,\n",
        "        head_num = 8,\n",
        "        drop_rate = 0.5\n",
        "    )\n",
        "    '''\n",
        "    def __init__(self, token_num, hidden_dim, heads_num, drop_rate=0.5):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        # 入力の線形変換\n",
        "        # 重み行列は[hidden_dim, hidden_dim]\n",
        "        self.query = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.key   = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.value = nn.Linear(hidden_dim, hidden_dim)\n",
        "        #self.query = nn.Conv1d(token_num, token_num, kernel_size=1)\n",
        "        #self.key   = nn.Conv1d(token_num, token_num, kernel_size=1)\n",
        "        #self.value = nn.Conv1d(token_num, token_num, kernel_size=1)\n",
        "        \n",
        "        # 出力の線形変換\n",
        "        self.projection = nn.Linear(hidden_dim, hidden_dim)\n",
        "        #self.projection = nn.Conv1d(token_num, token_num, kernel_size=1)\n",
        "        \n",
        "        # 出力のDropout\n",
        "        self.drop = nn.Dropout(drop_rate)\n",
        "        \n",
        "        self.nf = hidden_dim\n",
        "        self.nh = heads_num\n",
        "    \n",
        "    def atten(self, query, key, value, attention_mask):\n",
        "        \"\"\"\n",
        "        Attention\n",
        "        \n",
        "        query, key, value : 入力\n",
        "        attention_mask : attention weight に適用される mask\n",
        "        \"\"\"\n",
        "        # 各値を取得\n",
        "        shape = query.shape\n",
        "        batch_size = -1 if shape[0] is None else shape[0]\n",
        "        token_num = shape[2] # トークン列数\n",
        "        hidden_dim = shape[1]*shape[3] # 入力チャンネル数\n",
        "        \n",
        "        # ここで q と k の内積を取ることで、query と key の単語間の関連度のようなものを計算します。\n",
        "        # tf.matmulで最後の2成分について積を計算(それ以外は形がそろっている必要あり)\n",
        "        # transpose_bで転置\n",
        "        # [token_num, hidden_dim/head_num] @ [hidden_dim/head_num, token_num] = [token_num, token_num]\n",
        "        scores = torch.matmul(query, key.transpose(-2, -1))\n",
        "        \n",
        "        # scoreをhidden_dimの平方根割る\n",
        "        scores = scores / math.sqrt(hidden_dim)\n",
        "        \n",
        "        # Attention Maskがあればscoreに加算\n",
        "        # attention_mask: [batch_size, token_num, token_num] \n",
        "        # マスク(参照しない部分)の場所に1、使用する部分は0とする\n",
        "        # 0の部分を -無限大にする(softmax(-無限大)=0となる)\n",
        "        # 1. PADを無視\n",
        "        # 2. DecoderのSelf-Attentionで未来の情報を参照できないようにする\n",
        "        if attention_mask is not None:\n",
        "            scores = scores.masked_fill(attention_mask == 1, -1e9)\n",
        "\n",
        "        # softmax を取ることで正規化します\n",
        "        # input(query) の各単語に対して memory(key) の各単語のどこから情報を引いてくるかの重み\n",
        "        atten_weight = F.softmax(scores, dim = -1)\n",
        "        #atten_weight = scores / torch.sum(scores, dim=-1, keepdim=True)\n",
        "        \n",
        "        # 重みに従って value から情報を引いてきます\n",
        "        # [token_num, token_num] @ [token_num, hidden_dim/head_num] = [token_num, hidden_dim/head_num]\n",
        "        # input(query) の単語ごとに memory(value)の各単語 に attention_weight を掛け合わせて足し合わせた ベクトル(分散表現の重み付き和)を計算\n",
        "        context = torch.matmul(atten_weight, value)\n",
        "        \n",
        "        # 各ヘッドの結合(reshape)\n",
        "        # 入力と同じ形に変換する\n",
        "        context = context.transpose(1, 2).contiguous()\n",
        "        context = context.view(batch_size, token_num, hidden_dim)\n",
        "        \n",
        "        # 線形変換\n",
        "        context = self.projection(context)\n",
        "        \n",
        "        return self.drop(context), atten_weight\n",
        "\n",
        "    def _split(self, x):\n",
        "        \"\"\"\n",
        "        query, key, valueを分割する\n",
        "        \n",
        "        入力 shape: [batch_size, length, hidden_dim] の時\n",
        "        出力 shape: [batch_size, head_num, length, hidden_dim//head_num]\n",
        "        \"\"\"\n",
        "        # 各値を取得\n",
        "        hidden_dim = self.nf\n",
        "        heads_num = self.nh\n",
        "        shape = x.shape\n",
        "        batch_size = -1 if shape[0] is None else shape[0]\n",
        "        token_num = shape[1] # トークン列数\n",
        "        \n",
        "        # [batch_size, token_num, hidden_dim] -> [batch_size, token_num, head_num, hidden_dim/head_num]\n",
        "        # splitだが実際は次元を拡張する処理\n",
        "        x = x.view(batch_size, token_num, heads_num, int(hidden_dim/heads_num))\n",
        "        \n",
        "        # [batch_size, token_num, head_num, hidden_dim/head_num] -> [batch_size, head_num, token_num, hidden_dim/head_num]\n",
        "        x = x.transpose(1, 2)\n",
        "        return x\n",
        "    \n",
        "    def forward(self, x, memory=None, attention_mask=None, return_attention_scores=False):\n",
        "        \"\"\"\n",
        "        モデルの実行\n",
        "        \n",
        "        input : 入力(query) [batch_size, token_num, hidden_dim]\n",
        "        memory : 入力(key, value) [batch_size, token_num, hidden_dim]\n",
        "        attention_mask : attention weight に適用される mask\n",
        "            [batch_size, 1, q_length, k_length] \n",
        "            pad 等無視する部分が 1 となるようなもの(Decoderで使用)\n",
        "        \"\"\"\n",
        "        # memoryが入力されない場合、memory=input(Self Attention)とする\n",
        "        if memory is None:\n",
        "            memory = x\n",
        "        \n",
        "        # input -> query\n",
        "        # memory -> key, value\n",
        "        # [batch_size, token_num, hidden_dim] @ [hidden_dim, hidden_dim] -> [batch_size, token_num, hidden_dim] \n",
        "        query = self.query(x)\n",
        "        key = self.key(memory)\n",
        "        value = self.value(memory)\n",
        "        \n",
        "        # ヘッド数に分割する\n",
        "        # 実際はreshapeで次数を1つ増やす\n",
        "        # [batch_size, token_num, hidden_dim] -> [batch_size, head_num, token_num, hidden_dim/head_num]\n",
        "        query = self._split(query)\n",
        "        key = self._split(key)\n",
        "        value = self._split(value)\n",
        "        \n",
        "        # attention\n",
        "        # 入力と同じ形の出力\n",
        "        # context: [batch_size, token_num, hidden_dim]\n",
        "        # score_weightsはEncoderではNoneとする\n",
        "        context, atten_weight = self.atten(query, key, value, attention_mask)\n",
        "        \n",
        "        if return_attention_scores:\n",
        "            return context, atten_weight\n",
        "        else:\n",
        "            return context"
      ],
      "metadata": {
        "id": "bGBPe48UpLvi"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForwardNetwork(nn.Module):\n",
        "    '''\n",
        "    Position-wise Feedforward Neural Network\n",
        "    transformer blockで使用される全結合層\n",
        "    '''\n",
        "    def __init__(self, hidden_dim, drop_rate=0.1):\n",
        "        super().__init__()\n",
        "        # 2層構造\n",
        "        # 1層目：チャンネル数を増加させる\n",
        "        self.filter_dense_layer = nn.Linear(hidden_dim, hidden_dim * 4)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        \n",
        "        # 2層目：元のチャンネル数に戻す\n",
        "        self.output_dense_layer = nn.Linear(hidden_dim * 4, hidden_dim)\n",
        "        self.drop = nn.Dropout(drop_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        入力と出力で形が変わらない\n",
        "        [batch_size, token_num, hidden_dim]\n",
        "        '''\n",
        "        \n",
        "        # [batch_size, token_num, hidden_dim] -> [batch_size, token_num, 4*hidden_dim]\n",
        "        x = self.filter_dense_layer(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.drop(x)\n",
        "        \n",
        "        # [batch_size, token_num, 4*hidden_dim] -> [batch_size, token_num, hidden_dim]\n",
        "        return self.output_dense_layer(x)"
      ],
      "metadata": {
        "id": "9PErRogCpNs4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualNormalizationWrapper(nn.Module):\n",
        "    '''\n",
        "    残差接続\n",
        "    output: input + SubLayer(input)\n",
        "    '''\n",
        "    def __init__(self, hidden_dim, layer, drop_rate=0.1):\n",
        "        super().__init__()\n",
        "        self.layer = layer # SubLayer : ここではAttentionかFFN\n",
        "        self.layer_normalization = nn.LayerNorm(hidden_dim)\n",
        "        self.drop = nn.Dropout(drop_rate)\n",
        "\n",
        "    def forward(self, x, memory=None, attention_mask=None, return_attention_scores=False):\n",
        "        \"\"\"\n",
        "        AttentionもFFNも入力と出力で形が変わらない\n",
        "        [batch_size, token_num, hidden_dim]\n",
        "        \"\"\"\n",
        "        \n",
        "        params = {}\n",
        "        if memory is not None:\n",
        "            params['memory'] = memory\n",
        "        if attention_mask is not None:\n",
        "            params['attention_mask'] = attention_mask\n",
        "        if return_attention_scores:\n",
        "            params['return_attention_scores'] = return_attention_scores\n",
        "        \n",
        "        out = self.layer_normalization(x)\n",
        "        if return_attention_scores:\n",
        "            out, attn_weights = self.layer(out,**params)\n",
        "            out = self.drop(out)\n",
        "            return x + out, attn_weights\n",
        "        else:\n",
        "            out = self.layer(out,**params)\n",
        "            out = self.drop(out)\n",
        "            return x + out"
      ],
      "metadata": {
        "id": "Au-Ivln4pQS2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AddPositionalEncoding(nn.Module):\n",
        "    '''\n",
        "    入力テンソルに対し、位置の情報を付与して返すレイヤー\n",
        "    see: https://arxiv.org/pdf/1706.03762.pdf\n",
        "\n",
        "    PE_{pos, 2i}   = sin(pos / 10000^{2i / d_model})\n",
        "    PE_{pos, 2i+1} = cos(pos / 10000^{2i / d_model})\n",
        "    '''\n",
        "    def forward(self, inputs):\n",
        "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        fl_type = inputs.dtype\n",
        "        batch_size, max_length, depth = inputs.shape\n",
        "        \n",
        "        depth_counter = torch.div(torch.arange(depth) ,2, rounding_mode='trunc')*2\n",
        " \n",
        "        depth_matrix = torch.tile(torch.unsqueeze(depth_counter, 0), [max_length, 1])  # [max_length, depth]\n",
        "        depth_matrix = torch.pow(10000.0, depth_matrix / depth)  # [max_length, depth]\n",
        "        # cos(x) == sin(x + π/2)\n",
        "        phase = torch.remainder(torch.arange(depth), 2) * math.pi / 2\n",
        "        phase_matrix = torch.tile(torch.unsqueeze(phase, 0), [max_length, 1])  # [max_length, depth]\n",
        "\n",
        "        pos_counter = torch.arange(max_length)\n",
        "        pos_matrix = (torch.tile(torch.unsqueeze(pos_counter, 1), [1, depth]))  # [max_length, depth]\n",
        "\n",
        "        positional_encoding = torch.sin(pos_matrix / depth_matrix + phase_matrix)\n",
        "        # [batch_size, max_length, depth]\n",
        "        positional_encoding = torch.tile(torch.unsqueeze(positional_encoding, 0), [batch_size, 1, 1])\n",
        "        positional_encoding = positional_encoding.to(device)\n",
        "\n",
        "        return inputs + positional_encoding"
      ],
      "metadata": {
        "id": "hXpLDbkopWUK"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, pretrained_weight=None):\n",
        "        # vocab_size: 単語の総数\n",
        "        # embedding_dim: Embeddingの次数\n",
        "        super().__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        \n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=1)\n",
        "        self.embedding.weight.requires_grad = True\n",
        "        \n",
        "        if pretrained_weight is not None:\n",
        "            self.embedding.weight.data.copy_(pretrained_weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # inputのIDに対応したベクトルを持ってくる\n",
        "        embedding = self.embedding(x)\n",
        "        \n",
        "        return embedding * (self.embedding_dim ** 0.5)"
      ],
      "metadata": {
        "id": "E-KqLCACpYvC"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    transformer block : before ->[attention -> FF]-> next\n",
        "    それぞれ残差接続とLayerNormalizationの処理が含まれる\n",
        "    \"\"\"\n",
        "    def __init__(self, token_num, hidden_dim, heads_num, drop_rate=0.1):\n",
        "        \"\"\"\n",
        "        hidden_numはheads_numで割り切れえる値とすること\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.atten = ResidualNormalizationWrapper(\n",
        "            hidden_dim = hidden_dim,\n",
        "            layer = MultiHeadAttention(token_num=token_num, hidden_dim = hidden_dim, heads_num = heads_num, drop_rate = drop_rate),\n",
        "            drop_rate = drop_rate)\n",
        "        \n",
        "        self.ffn = ResidualNormalizationWrapper(\n",
        "            hidden_dim = hidden_dim,\n",
        "            layer = FeedForwardNetwork(hidden_dim = hidden_dim, drop_rate = drop_rate),\n",
        "            drop_rate = drop_rate)\n",
        "    \n",
        "    def forward(self, input, memory=None, attention_mask=None, return_attention_scores=False):\n",
        "        \"\"\"\n",
        "        入力と出力で形式が変わらない\n",
        "        [batch_size, token_num, hidden_dim]\n",
        "        \"\"\"\n",
        "        if return_attention_scores:\n",
        "            x, attn_weights = self.atten(input, memory, attention_mask, return_attention_scores)\n",
        "            x = self.ffn(x)\n",
        "            return x, attn_weights\n",
        "        else:\n",
        "            x = self.atten(input, memory, attention_mask, return_attention_scores)\n",
        "            x = self.ffn(x)\n",
        "            return x"
      ],
      "metadata": {
        "id": "ILkSHm6NpcSe"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    '''\n",
        "    TransformerのEncoder\n",
        "    '''\n",
        "    def __init__(\n",
        "            self,\n",
        "            vocab_size, # 単語の総数\n",
        "            hopping_num, # Multi-head Attentionの繰り返し数\n",
        "            heads_num, # Multi-head Attentionのヘッド数\n",
        "            hidden_dim, # Embeddingの次数\n",
        "            token_num, # 系列長(文章中のトークン数)\n",
        "            drop_rate, # ドロップアウトの確率\n",
        "            pretrained_weight=None\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.hopping_num = hopping_num\n",
        "        \n",
        "        # Embedding層\n",
        "        self.token_embedding = TokenEmbedding(vocab_size, hidden_dim, pretrained_weight)\n",
        "        # Position Embedding\n",
        "        self.add_position_embedding = AddPositionalEncoding()\n",
        "        self.input_dropout_layer = nn.Dropout(drop_rate)\n",
        "\n",
        "        # Multi-head Attentionの繰り返し(hopping)のリスト\n",
        "        self.attention_block_list = nn.ModuleList([TransformerBlock(token_num, hidden_dim, heads_num) for _ in range(hopping_num)])\n",
        "        self.output_normalization = nn.LayerNorm(hidden_dim)\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            input,\n",
        "            memory=None,\n",
        "            attention_mask=None,\n",
        "            return_attention_scores=False\n",
        "    ):\n",
        "        '''\n",
        "        input: 入力 [batch_size, length]\n",
        "        memory: 入力 [batch_size, length]\n",
        "        attention_mask: attention weight に適用される mask\n",
        "            [batch_size, 1, q_length, k_length] \n",
        "            pad 等無視する部分が 0 となるようなもの(Decoderで使用)\n",
        "        出力 [batch_size, length, hidden_dim]\n",
        "        '''\n",
        "        # [batch_size, token_num] -> [batch_size, token_num, hidden_dim]\n",
        "        embedded_input = self.token_embedding(input)\n",
        "        # Positional Embedding\n",
        "        embedded_input = self.add_position_embedding(embedded_input)\n",
        "        query = self.input_dropout_layer(embedded_input)\n",
        "        \n",
        "        if return_attention_scores:\n",
        "            # MultiHead Attentionを繰り返し適用\n",
        "            for i in range(self.hopping_num):\n",
        "                query, atten_weights = self.attention_block_list[i](query, memory, attention_mask, return_attention_scores)\n",
        "\n",
        "            # [batch_size, token_num, hidden_dim]\n",
        "            return self.output_normalization(query), atten_weights\n",
        "        else:\n",
        "            # MultiHead Attentionを繰り返し適用\n",
        "            for i in range(self.hopping_num):\n",
        "                query = self.attention_block_list[i](query, memory, attention_mask, return_attention_scores)\n",
        "\n",
        "            # [batch_size, token_num, hidden_dim]\n",
        "            return self.output_normalization(query)"
      ],
      "metadata": {
        "id": "dbGtiT3Npeyq"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionClassifier(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            vocab_size, # 単語の総数\n",
        "            hopping_num, # Multi-head Attentionの繰り返し数\n",
        "            heads_num, # Multi-head Attentionのヘッド数\n",
        "            hidden_dim, # Embeddingの次数\n",
        "            token_num, # 系列長(文章中のトークン数)\n",
        "            drop_rate, # ドロップアウトの確率\n",
        "            NUMLABELS, # クラス数\n",
        "            pretrained_weight=None,\n",
        "            PAD_ID = 1\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.PAD_ID = PAD_ID\n",
        "        \n",
        "        self.encoder = Encoder(vocab_size, hopping_num, heads_num, hidden_dim, token_num, drop_rate, pretrained_weight)\n",
        "        self.dense1 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.act1 = nn.Tanh()\n",
        "        self.dropout1 = nn.Dropout(drop_rate)   \n",
        "        self.final_layer = nn.Linear(hidden_dim, NUMLABELS)\n",
        "        \n",
        "        nn.init.normal_(self.dense1.weight, std=0.02)\n",
        "        nn.init.normal_(self.dense1.bias, std=0)\n",
        "        nn.init.normal_(self.final_layer.weight, std=0.02)\n",
        "        nn.init.normal_(self.final_layer.bias, std=0)\n",
        "\n",
        "    def forward(self, x, return_attention_scores=False):\n",
        "        self_attention_mask=self._create_enc_attention_mask(x)\n",
        "        \n",
        "        # [batch_size, token_num] -> [batch_size, token_num, hidden_dim]\n",
        "        if return_attention_scores:\n",
        "            enc_output, atten_weights = self.encoder(x, attention_mask=self_attention_mask,return_attention_scores=return_attention_scores)\n",
        "        else:\n",
        "            enc_output = self.encoder(x, attention_mask=self_attention_mask,return_attention_scores=return_attention_scores)\n",
        "        \n",
        "        # 文頭の重みを使用 [batch_size, 0, hidden_dim]\n",
        "        # [batch_size, hidden_dim] -> [batch_size, hidden_dim]\n",
        "        enc_output = self.dense1(enc_output[:, 0, :])\n",
        "        enc_output = self.act1(enc_output)\n",
        "        enc_output = self.dropout1(enc_output)\n",
        "        \n",
        "        # [batch_size, hidden_dim] -> [batch_size, NUMLABELS]\n",
        "        final_output = self.final_layer(enc_output)\n",
        "\n",
        "        if return_attention_scores:\n",
        "            return final_output, atten_weights\n",
        "        else:\n",
        "            return final_output\n",
        "    \n",
        "    def _create_enc_attention_mask(self, x):\n",
        "        batch_size, length = x.shape\n",
        "        # マスクする部分を1とする\n",
        "        pad_array = torch.eq(x, self.PAD_ID).to(dtype=torch.int8)  # [batch_size, token_num]\n",
        "        \n",
        "        # shape broadcasting で [batch_size, head_num, token_num, token_num] になる\n",
        "        return pad_array.view([batch_size, 1, 1, length])"
      ],
      "metadata": {
        "id": "VR0iN6obpgho"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install janome\n",
        "import re\n",
        "from janome.tokenizer import Tokenizer\n",
        "j_t = Tokenizer(wakati=True)\n",
        "\n",
        "def tokenizer_janome(text):\n",
        "    return [tok for tok in j_t.tokenize(text, wakati=True)]\n",
        "\n",
        "def preprocessing_text(text):\n",
        "    text = re.sub('\\r', '', text)\n",
        "    text = re.sub('\\n', '', text)\n",
        "    text = re.sub('　', '', text)\n",
        "    text = re.sub(' ', '', text)\n",
        "    \n",
        "    text = re.sub(r'[0-9 ０-９]', '0', text)\n",
        "    return text\n",
        "\n",
        "def tokenizer_with_preprocessing(text):\n",
        "    text = preprocessing_text(text)\n",
        "    ret = tokenizer_janome(text)\n",
        "    return ret"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zB6bBB7Kpivf",
        "outputId": "d7c153ae-e73b-4bb8-f625-82adfb57a490"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting janome\n",
            "  Downloading Janome-0.4.2-py2.py3-none-any.whl (19.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.7 MB 1.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: janome\n",
            "Successfully installed janome-0.4.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchtext\n",
        "#from torchtext import data, datasets\n",
        "from torchtext.legacy import data\n",
        "\n",
        "max_length = 64\n",
        "TEXT = data.Field(sequential=True, tokenize=tokenizer_with_preprocessing,\n",
        "                  use_vocab=True, lower=True, include_lengths=True,\n",
        "                  batch_first=True, fix_length=max_length,init_token=\"<eos>\",eos_token=\"<cls>\")\n",
        "LABEL = data.Field(sequential=False, use_vocab=False, preprocessing=None)\n",
        "\n",
        "dataset = data.TabularDataset(\n",
        "        path='reviews.csv', format='csv',\n",
        "        skip_header=True,\n",
        "        fields=[('Text', TEXT), ('Label', LABEL), ('Label2', LABEL)])\n",
        "\n",
        "train_dataset, test_dataset = dataset.split(split_ratio=0.7)\n",
        "train_dataset, val_dataset = train_dataset.split(split_ratio=0.7)\n",
        "\n",
        "\n",
        "\n",
        "#train_iter, val_iter, test_iter = data.BucketIterator.splits((train_dataset, val_dataset, test_dataset), batch_size=32, repeat=False, shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        },
        "id": "EkFR_uKXprbG",
        "outputId": "0cccaa33-7c10-4740-87ba-f87bb364e66e"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-f6901be5273c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m train_iter = torchtext.data.Iterator(\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m  \u001b[0;31m# train=Trueならシャッフルソートは有効\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'torchtext.data' has no attribute 'Iterator'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_iter = data.Iterator(\n",
        "    train_dataset, batch_size=32, \n",
        "    train=True  # train=Trueならシャッフルソートは有効\n",
        ")\n",
        "val_iter = data.Iterator(\n",
        "    val_dataset, batch_size=32, \n",
        "    train=False, sort=False\n",
        ")\n",
        "test_iter = data.Iterator(\n",
        "    test_dataset, batch_size=32, \n",
        "    train=False, sort=False\n",
        ")"
      ],
      "metadata": {
        "id": "68lMwWJmr8md"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TEXT.build_vocab(train_dataset)\n",
        "vocab = TEXT.vocab\n",
        "len(vocab)\n",
        "\n",
        "batch = next(iter(train_iter))\n",
        "\n",
        "net = AttentionClassifier(\n",
        "            vocab_size = len(vocab), # 単語の総数\n",
        "            hopping_num = 8, # Multi-head Attentionの繰り返し数\n",
        "            heads_num = 6, # Multi-head Attentionのヘッド数\n",
        "            hidden_dim = 300, # Embeddingの次数\n",
        "            drop_rate = 0.1, # ドロップアウトの確率\n",
        "            token_num = 64,\n",
        "            \n",
        "    pretrained_weight=None,\n",
        "    NUMLABELS=2\n",
        "    )\n",
        "\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Linear') != -1:\n",
        "        nn.init.kaiming_normal_(m.weight)\n",
        "        if m.bias is not None:\n",
        "            nn.init.constant_(m.bias, 0.0)\n",
        "net.train()\n",
        "\n",
        "net.apply(weights_init)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8uE9SQ3fplSc",
        "outputId": "4a0b5427-4e9e-42aa-f1da-949f152f17ce"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AttentionClassifier(\n",
              "  (encoder): Encoder(\n",
              "    (token_embedding): TokenEmbedding(\n",
              "      (embedding): Embedding(10582, 300, padding_idx=1)\n",
              "    )\n",
              "    (add_position_embedding): AddPositionalEncoding()\n",
              "    (input_dropout_layer): Dropout(p=0.1, inplace=False)\n",
              "    (attention_block_list): ModuleList(\n",
              "      (0): TransformerBlock(\n",
              "        (atten): ResidualNormalizationWrapper(\n",
              "          (layer): MultiHeadAttention(\n",
              "            (query): Linear(in_features=300, out_features=300, bias=True)\n",
              "            (key): Linear(in_features=300, out_features=300, bias=True)\n",
              "            (value): Linear(in_features=300, out_features=300, bias=True)\n",
              "            (projection): Linear(in_features=300, out_features=300, bias=True)\n",
              "            (drop): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (layer_normalization): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
              "          (drop): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ffn): ResidualNormalizationWrapper(\n",
              "          (layer): FeedForwardNetwork(\n",
              "            (filter_dense_layer): Linear(in_features=300, out_features=1200, bias=True)\n",
              "            (relu1): ReLU()\n",
              "            (output_dense_layer): Linear(in_features=1200, out_features=300, bias=True)\n",
              "            (drop): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (layer_normalization): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
              "          (drop): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): TransformerBlock(\n",
              "        (atten): ResidualNormalizationWrapper(\n",
              "          (layer): MultiHeadAttention(\n",
              "            (query): Linear(in_features=300, out_features=300, bias=True)\n",
              "            (key): Linear(in_features=300, out_features=300, bias=True)\n",
              "            (value): Linear(in_features=300, out_features=300, bias=True)\n",
              "            (projection): Linear(in_features=300, out_features=300, bias=True)\n",
              "            (drop): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (layer_normalization): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
              "          (drop): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ffn): ResidualNormalizationWrapper(\n",
              "          (layer): FeedForwardNetwork(\n",
              "            (filter_dense_layer): Linear(in_features=300, out_features=1200, bias=True)\n",
              "            (relu1): ReLU()\n",
              "            (output_dense_layer): Linear(in_features=1200, out_features=300, bias=True)\n",
              "            (drop): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (layer_normalization): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
              "          (drop): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (2): TransformerBlock(\n",
              "        (atten): ResidualNormalizationWrapper(\n",
              "          (layer): MultiHeadAttention(\n",
              "            (query): Linear(in_features=300, out_features=300, bias=True)\n",
              "            (key): Linear(in_features=300, out_features=300, bias=True)\n",
              "            (value): Linear(in_features=300, out_features=300, bias=True)\n",
              "            (projection): Linear(in_features=300, out_features=300, bias=True)\n",
              "            (drop): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (layer_normalization): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
              "          (drop): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ffn): ResidualNormalizationWrapper(\n",
              "          (layer): FeedForwardNetwork(\n",
              "            (filter_dense_layer): Linear(in_features=300, out_features=1200, bias=True)\n",
              "            (relu1): ReLU()\n",
              "            (output_dense_layer): Linear(in_features=1200, out_features=300, bias=True)\n",
              "            (drop): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (layer_normalization): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
              "          (drop): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (3): TransformerBlock(\n",
              "        (atten): ResidualNormalizationWrapper(\n",
              "          (layer): MultiHeadAttention(\n",
              "            (query): Linear(in_features=300, out_features=300, bias=True)\n",
              "            (key): Linear(in_features=300, out_features=300, bias=True)\n",
              "            (value): Linear(in_features=300, out_features=300, bias=True)\n",
              "            (projection): Linear(in_features=300, out_features=300, bias=True)\n",
              "            (drop): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (layer_normalization): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
              "          (drop): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ffn): ResidualNormalizationWrapper(\n",
              "          (layer): FeedForwardNetwork(\n",
              "            (filter_dense_layer): Linear(in_features=300, out_features=1200, bias=True)\n",
              "            (relu1): ReLU()\n",
              "            (output_dense_layer): Linear(in_features=1200, out_features=300, bias=True)\n",
              "            (drop): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (layer_normalization): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
              "          (drop): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (4): TransformerBlock(\n",
              "        (atten): ResidualNormalizationWrapper(\n",
              "          (layer): MultiHeadAttention(\n",
              "            (query): Linear(in_features=300, out_features=300, bias=True)\n",
              "            (key): Linear(in_features=300, out_features=300, bias=True)\n",
              "            (value): Linear(in_features=300, out_features=300, bias=True)\n",
              "            (projection): Linear(in_features=300, out_features=300, bias=True)\n",
              "            (drop): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (layer_normalization): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
              "          (drop): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ffn): ResidualNormalizationWrapper(\n",
              "          (layer): FeedForwardNetwork(\n",
              "            (filter_dense_layer): Linear(in_features=300, out_features=1200, bias=True)\n",
              "            (relu1): ReLU()\n",
              "            (output_dense_layer): Linear(in_features=1200, out_features=300, bias=True)\n",
              "            (drop): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (layer_normalization): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
              "          (drop): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (5): TransformerBlock(\n",
              "        (atten): ResidualNormalizationWrapper(\n",
              "          (layer): MultiHeadAttention(\n",
              "            (query): Linear(in_features=300, out_features=300, bias=True)\n",
              "            (key): Linear(in_features=300, out_features=300, bias=True)\n",
              "            (value): Linear(in_features=300, out_features=300, bias=True)\n",
              "            (projection): Linear(in_features=300, out_features=300, bias=True)\n",
              "            (drop): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (layer_normalization): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
              "          (drop): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ffn): ResidualNormalizationWrapper(\n",
              "          (layer): FeedForwardNetwork(\n",
              "            (filter_dense_layer): Linear(in_features=300, out_features=1200, bias=True)\n",
              "            (relu1): ReLU()\n",
              "            (output_dense_layer): Linear(in_features=1200, out_features=300, bias=True)\n",
              "            (drop): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (layer_normalization): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
              "          (drop): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (6): TransformerBlock(\n",
              "        (atten): ResidualNormalizationWrapper(\n",
              "          (layer): MultiHeadAttention(\n",
              "            (query): Linear(in_features=300, out_features=300, bias=True)\n",
              "            (key): Linear(in_features=300, out_features=300, bias=True)\n",
              "            (value): Linear(in_features=300, out_features=300, bias=True)\n",
              "            (projection): Linear(in_features=300, out_features=300, bias=True)\n",
              "            (drop): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (layer_normalization): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
              "          (drop): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ffn): ResidualNormalizationWrapper(\n",
              "          (layer): FeedForwardNetwork(\n",
              "            (filter_dense_layer): Linear(in_features=300, out_features=1200, bias=True)\n",
              "            (relu1): ReLU()\n",
              "            (output_dense_layer): Linear(in_features=1200, out_features=300, bias=True)\n",
              "            (drop): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (layer_normalization): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
              "          (drop): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (7): TransformerBlock(\n",
              "        (atten): ResidualNormalizationWrapper(\n",
              "          (layer): MultiHeadAttention(\n",
              "            (query): Linear(in_features=300, out_features=300, bias=True)\n",
              "            (key): Linear(in_features=300, out_features=300, bias=True)\n",
              "            (value): Linear(in_features=300, out_features=300, bias=True)\n",
              "            (projection): Linear(in_features=300, out_features=300, bias=True)\n",
              "            (drop): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (layer_normalization): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
              "          (drop): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ffn): ResidualNormalizationWrapper(\n",
              "          (layer): FeedForwardNetwork(\n",
              "            (filter_dense_layer): Linear(in_features=300, out_features=1200, bias=True)\n",
              "            (relu1): ReLU()\n",
              "            (output_dense_layer): Linear(in_features=1200, out_features=300, bias=True)\n",
              "            (drop): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (layer_normalization): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
              "          (drop): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (output_normalization): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (dense1): Linear(in_features=300, out_features=300, bias=True)\n",
              "  (act1): Tanh()\n",
              "  (dropout1): Dropout(p=0.1, inplace=False)\n",
              "  (final_layer): Linear(in_features=300, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "learning_rate = 2e-4\n",
        "optimizer = optimizers.Adam(net.parameters(), lr=learning_rate, amsgrad=True, eps=1e-07)"
      ],
      "metadata": {
        "id": "XiGrOv-tqNlg"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(net, dataloaders_dict, criterion, optimizer, num_epochs):\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"使用デバイス:\", device)\n",
        "    print('--------start--------')\n",
        "    net.to(device)\n",
        "    \n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                net.train()\n",
        "            else:\n",
        "                net.eval()\n",
        "            \n",
        "            epoch_loss = 0.0\n",
        "            epoch_corrects = 0\n",
        "            \n",
        "            for batch in (dataloaders_dict[phase]):\n",
        "                inputs = batch.Text[0].to(device)\n",
        "                labels = batch.Label2.to(device)\n",
        "                \n",
        "                optimizer.zero_grad()\n",
        "                \n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = net(inputs)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    \n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    \n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "                    \n",
        "                    epoch_loss += loss.item() * inputs.size(0)\n",
        "                    epoch_corrects += torch.sum(preds == labels.data)\n",
        "            \n",
        "            epoch_loss = epoch_loss / len(dataloaders_dict[phase].dataset)\n",
        "            epoch_acc = epoch_corrects.double() / len(dataloaders_dict[phase].dataset)\n",
        "            \n",
        "            print('Epoch {}/{} | {:.^5} | Loss: {:.4f} Acc: {:.4f}'.format(epoch+1,\n",
        "                                                                           num_epochs,\n",
        "                                                                           phase,\n",
        "                                                                           epoch_loss,\n",
        "                                                                           epoch_acc))\n",
        "        \n",
        "    return net"
      ],
      "metadata": {
        "id": "KKY-VaeiqQC2"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloaders_dict = {\"train\":train_iter, \"val\":val_iter}"
      ],
      "metadata": {
        "id": "WV_SobWyqSIZ"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 20\n",
        "\n",
        "net_trained = train_model(net, dataloaders_dict, criterion, optimizer, num_epochs=num_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQHE96E6qXlH",
        "outputId": "d95c35aa-8e25-4b0f-82a8-59e453919898"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "使用デバイス: cuda:0\n",
            "--------start--------\n",
            "Epoch 1/20 | train | Loss: 0.5432 Acc: 0.7392\n",
            "Epoch 1/20 | .val. | Loss: 0.4456 Acc: 0.7806\n",
            "Epoch 2/20 | train | Loss: 0.4326 Acc: 0.8147\n",
            "Epoch 2/20 | .val. | Loss: 0.4013 Acc: 0.8277\n",
            "Epoch 3/20 | train | Loss: 0.3595 Acc: 0.8526\n",
            "Epoch 3/20 | .val. | Loss: 0.3760 Acc: 0.8352\n",
            "Epoch 4/20 | train | Loss: 0.3004 Acc: 0.8748\n",
            "Epoch 4/20 | .val. | Loss: 0.3251 Acc: 0.8653\n",
            "Epoch 5/20 | train | Loss: 0.2488 Acc: 0.8962\n",
            "Epoch 5/20 | .val. | Loss: 0.3283 Acc: 0.8814\n",
            "Epoch 6/20 | train | Loss: 0.2029 Acc: 0.9225\n",
            "Epoch 6/20 | .val. | Loss: 0.3071 Acc: 0.8804\n",
            "Epoch 7/20 | train | Loss: 0.1777 Acc: 0.9294\n",
            "Epoch 7/20 | .val. | Loss: 0.3506 Acc: 0.8804\n",
            "Epoch 8/20 | train | Loss: 0.1009 Acc: 0.9657\n",
            "Epoch 8/20 | .val. | Loss: 0.5227 Acc: 0.8625\n",
            "Epoch 9/20 | train | Loss: 0.0666 Acc: 0.9766\n",
            "Epoch 9/20 | .val. | Loss: 0.4938 Acc: 0.8766\n",
            "Epoch 10/20 | train | Loss: 0.0345 Acc: 0.9903\n",
            "Epoch 10/20 | .val. | Loss: 0.6993 Acc: 0.8550\n",
            "Epoch 11/20 | train | Loss: 0.0669 Acc: 0.9782\n",
            "Epoch 11/20 | .val. | Loss: 0.6450 Acc: 0.8390\n",
            "Epoch 12/20 | train | Loss: 0.0414 Acc: 0.9859\n",
            "Epoch 12/20 | .val. | Loss: 0.5674 Acc: 0.8832\n",
            "Epoch 13/20 | train | Loss: 0.0153 Acc: 0.9948\n",
            "Epoch 13/20 | .val. | Loss: 0.6778 Acc: 0.8804\n",
            "Epoch 14/20 | train | Loss: 0.0075 Acc: 0.9984\n",
            "Epoch 14/20 | .val. | Loss: 0.7238 Acc: 0.8776\n",
            "Epoch 15/20 | train | Loss: 0.0064 Acc: 0.9980\n",
            "Epoch 15/20 | .val. | Loss: 0.7234 Acc: 0.8814\n",
            "Epoch 16/20 | train | Loss: 0.0105 Acc: 0.9960\n",
            "Epoch 16/20 | .val. | Loss: 0.8121 Acc: 0.8738\n",
            "Epoch 17/20 | train | Loss: 0.0089 Acc: 0.9960\n",
            "Epoch 17/20 | .val. | Loss: 0.8237 Acc: 0.8804\n",
            "Epoch 18/20 | train | Loss: 0.0128 Acc: 0.9952\n",
            "Epoch 18/20 | .val. | Loss: 0.8530 Acc: 0.8851\n",
            "Epoch 19/20 | train | Loss: 0.0111 Acc: 0.9960\n",
            "Epoch 19/20 | .val. | Loss: 0.8995 Acc: 0.8710\n",
            "Epoch 20/20 | train | Loss: 0.0142 Acc: 0.9948\n",
            "Epoch 20/20 | .val. | Loss: 0.9120 Acc: 0.8757\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "def mk_html(index, batch, preds, normlized_weights, vocab, labels=[\"Negative\", \"Positive\"]):\n",
        "    \"HTMLデータを作成する\"\n",
        "\n",
        "    # indexの結果を抽出\n",
        "    sentence = batch[0][index]  # 文章\n",
        "    label = batch[1][index] # ラベル\n",
        "    pred = preds[index]  # 予測\n",
        "    # ラベルと予測結果を文字に置き換え\n",
        "    label_str = labels[label]\n",
        "    pred_str = labels[pred]\n",
        "    # 表示用のHTMLを作成する\n",
        "    html = '正解ラベル：{}<br>推論ラベル：{}<br><br>'.format(label_str, pred_str)\n",
        "\n",
        "    # 12種類のAttentionの平均を求める。最大値で規格化\n",
        "    all_attens = normlized_weights[0, :, 0, :].sum(axis=0)*0  # all_attensという変数を作成する\n",
        "    all_attens = np.sum(normlized_weights[index, :, 0, :], axis=0)\n",
        "    all_attens = (all_attens -all_attens.min()) /  (all_attens.max()-all_attens.min())\n",
        "\n",
        "    for word, attn in zip(sentence, all_attens):\n",
        "        # 単語が[SEP]の場合は文章が終わりなのでbreak\n",
        "        if TEXT.vocab.itos[word] == \"<cls>\":\n",
        "            break\n",
        "\n",
        "        # 関数highlightで色をつける、関数tokenizer_bert.convert_ids_to_tokensでIDを単語に戻す\n",
        "        html += highlight(TEXT.vocab.itos[word], attn)\n",
        "    html += \"<br><br>\"\n",
        "\n",
        "    return html\n",
        "\n",
        "def highlight(word, attn):\n",
        "    \"Attentionの値が大きいと文字の背景が濃い赤になるhtmlを出力させる関数\"\n",
        "\n",
        "    html_color = '#%02X%02X%02X' % (\n",
        "        255, int(255*(1 - attn)), int(255*(1 - attn)))\n",
        "    return '<span style=\"background-color: {}\"> {}</span>'.format(html_color, word)"
      ],
      "metadata": {
        "id": "2TCVgj0Euw4y"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_html(preds, batch, labels=[\"Negative\", \"Positive\"]):\n",
        "  html_output = [mk_html(index=idx,\n",
        "                         batch=batch, \n",
        "                         preds=np.argmax(preds, axis=1),\n",
        "                         normlized_weights=atten,\n",
        "                         vocab=vocab,\n",
        "                         labels=labels) for idx in np.arange(len(preds))]\n",
        "  return  html_output"
      ],
      "metadata": {
        "id": "aLkt9_psuxut"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "batch = next(iter(test_iter))\n",
        "\n",
        "preds, atten = net(batch.Text[0].to(device), return_attention_scores=True)\n",
        "atten = atten.to('cpu').detach().numpy()\n",
        "preds = preds.to('cpu').detach().numpy()"
      ],
      "metadata": {
        "id": "D0ZGiH-_u0Y6"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "html_results = make_html(preds, batch=(np.array(batch.Text[0]), np.array(batch.Label2)), labels=[\"Negative\", \"Positive\"])"
      ],
      "metadata": {
        "id": "o4l4LL06vN2z"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "HTML(html_results[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "FBboOgswvrRj",
        "outputId": "c07e02bf-51c7-4b1e-c4ec-8368b170e703"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "正解ラベル：Positive<br>推論ラベル：Positive<br><br><span style=\"background-color: #FFFEFE\"> <eos></span><span style=\"background-color: #FFF6F6\"> 日曜日</span><span style=\"background-color: #FFFEFE\"> に</span><span style=\"background-color: #FFFCFC\"> 利用</span><span style=\"background-color: #FFFEFE\"> し</span><span style=\"background-color: #FFFEFE\"> まし</span><span style=\"background-color: #FFFEFE\"> た</span><span style=\"background-color: #FFFEFE\"> 。</span><span style=\"background-color: #FF0B0B\"> 週末</span><span style=\"background-color: #FFFDFD\"> でも</span><span style=\"background-color: #FF1212\"> 遅く</span><span style=\"background-color: #FFFDFD\"> まで</span><span style=\"background-color: #FFFDFD\"> 営業</span><span style=\"background-color: #FFFEFE\"> し</span><span style=\"background-color: #FFFEFE\"> て</span><span style=\"background-color: #FFFEFE\"> いる</span><span style=\"background-color: #FFFEFE\"> の</span><span style=\"background-color: #FFFEFE\"> は</span><span style=\"background-color: #FFD8D8\"> ありがたい</span><span style=\"background-color: #FFFEFE\"> です</span><span style=\"background-color: #FFFEFE\"> 。</span><span style=\"background-color: #FFFEFE\"> 若者</span><span style=\"background-color: #FF0000\"> で</span><span style=\"background-color: #FFFBFB\"> 溢れ</span><span style=\"background-color: #FFFEFE\"> <unk></span><span style=\"background-color: #FFFEFE\"> 、</span><span style=\"background-color: #FFFCFC\"> 道</span><span style=\"background-color: #FFFEFE\"> が</span><span style=\"background-color: #FFFEFE\"> 狭く</span><span style=\"background-color: #FFFBFB\"> 雑多</span><span style=\"background-color: #FFFEFE\"> な</span><span style=\"background-color: #FFA6A6\"> 雰囲気</span><span style=\"background-color: #FFFEFE\"> の</span><span style=\"background-color: #FFFDFD\"> 下北沢</span><span style=\"background-color: #FFFEFE\"> です</span><span style=\"background-color: #FFFEFE\"> が</span><span style=\"background-color: #FFFEFE\"> 、</span><span style=\"background-color: #FFF6F6\"> 通り</span><span style=\"background-color: #FFFAFA\"> から</span><span style=\"background-color: #FF2828\"> 少し</span><span style=\"background-color: #FF0B0B\"> 入れ</span><span style=\"background-color: #FFF5F5\"> ば</span><span style=\"background-color: #FFAAAA\"> こんなに</span><span style=\"background-color: #FFF4F4\"> 落ち着い</span><span style=\"background-color: #FFFEFE\"> た</span><span style=\"background-color: #FFD3D3\"> 空間</span><span style=\"background-color: #FFFEFE\"> が</span><span style=\"background-color: #FFFEFE\"> ある</span><span style=\"background-color: #FFFEFE\"> と</span><span style=\"background-color: #FFFEFE\"> は</span><span style=\"background-color: #FFF7F7\"> 驚き</span><span style=\"background-color: #FFFEFE\"> です</span><span style=\"background-color: #FFFEFE\"> 。</span><span style=\"background-color: #FFF5F5\"> 大きな</span><span style=\"background-color: #FFFEFE\"> ガラス</span><span style=\"background-color: #FFFFFF\"> 窓</span><span style=\"background-color: #FFFEFE\"> が</span><span style=\"background-color: #FFFEFE\"> ある</span><span style=\"background-color: #FFFEFE\"> 店内</span><span style=\"background-color: #FFFEFE\"> は</span><span style=\"background-color: #FFFDFD\"> 開放</span><span style=\"background-color: #FFFCFC\"> 感</span><span style=\"background-color: #FFFEFE\"> が</span><br><br>"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "9ob3lsMCvOXR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}