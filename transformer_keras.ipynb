{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled11.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMaJV9fGzO01LGoNHkT4mD9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tanakakao/test/blob/main/transformer_keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9CI07-NB-bJ4"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Layer, Input, Dense, Conv1D, Activation, Dropout, LayerNormalization, Reshape, Embedding, MultiHeadAttention\n",
        "from tensorflow.keras import activations\n",
        "import numpy as np\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForwardNetwork(Layer):\n",
        "    '''\n",
        "    Position-wise Feedforward Neural Network\n",
        "    transformer blockで使用される全結合層\n",
        "    '''\n",
        "    def __init__(self, hidden_dim, drop_rate):\n",
        "        super().__init__()\n",
        "        # 2層構造\n",
        "        # 1層目：チャンネル数を増加させる\n",
        "        self.filter_dense_layer = Dense(hidden_dim * 4, use_bias=True, activation='relu')\n",
        "        \n",
        "        # 2層目：元のチャンネル数に戻す\n",
        "        self.output_dense_layer = Dense(hidden_dim, use_bias=True)\n",
        "        self.drop = Dropout(drop_rate)\n",
        "\n",
        "    def call(self, x, training):\n",
        "        '''\n",
        "        入力と出力で形が変わらない\n",
        "        [batch_size, token_num, hidden_dim]\n",
        "        '''\n",
        "        \n",
        "        # [batch_size, token_num, hidden_dim] -> [batch_size, token_num, 4*hidden_dim]\n",
        "        x = self.filter_dense_layer(x)\n",
        "        x = self.drop(x, training=training)\n",
        "        \n",
        "        # [batch_size, token_num, 4*hidden_dim] -> [batch_size, token_num, hidden_dim]\n",
        "        return self.output_dense_layer(x)"
      ],
      "metadata": {
        "id": "SWl6hpOk-i_0"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualNormalizationWrapper(Layer):\n",
        "    '''\n",
        "    残差接続\n",
        "    output: input + SubLayer(input)\n",
        "    '''\n",
        "    def __init__(self, layer, drop_rate):\n",
        "        super().__init__()\n",
        "        self.layer = layer # SubLayer : ここではAttentionかFFN\n",
        "        self.layer_normalization = LayerNormalization()\n",
        "        self.drop = Dropout(drop_rate)\n",
        "\n",
        "    def call(self, x, training, value=None, attention_mask=None, return_attention_scores=None):\n",
        "        \"\"\"\n",
        "        AttentionもFFNも入力と出力で形が変わらない\n",
        "        [batch_size, token_num, hidden_dim]\n",
        "        \"\"\"\n",
        "        \n",
        "        params = {}\n",
        "        if attention_mask is not None:\n",
        "            params['attention_mask'] = attention_mask\n",
        "        if return_attention_scores:\n",
        "            params['return_attention_scores'] = return_attention_scores\n",
        "        \n",
        "        out = self.layer_normalization(x)\n",
        "        if value is not None:\n",
        "            params['value'] = out\n",
        "\n",
        "        if return_attention_scores:\n",
        "            out, attn_weights = self.layer(out,training=training, **params)\n",
        "            out = self.drop(out, training=training)\n",
        "            return x + out, attn_weights\n",
        "        else:\n",
        "            out = self.layer(out,training=training, **params)\n",
        "            out = self.drop(out, training=training)\n",
        "            return x + out"
      ],
      "metadata": {
        "id": "5oSyAN_4_IC5"
      },
      "execution_count": 223,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AddPositionalEncoding(Layer):\n",
        "    '''\n",
        "    入力テンソルに対し、位置の情報を付与して返すレイヤー\n",
        "    see: https://arxiv.org/pdf/1706.03762.pdf\n",
        "\n",
        "    PE_{pos, 2i}   = sin(pos / 10000^{2i / d_model})\n",
        "    PE_{pos, 2i+1} = cos(pos / 10000^{2i / d_model})\n",
        "    '''\n",
        "    def call(self, inputs):\n",
        "        fl_type = inputs.dtype\n",
        "        batch_size, max_length, depth = tf.unstack(tf.shape(inputs))\n",
        "\n",
        "        depth_counter = tf.range(depth) // 2 * 2  # 0, 0, 2, 2, 4, ...\n",
        "        depth_matrix = tf.tile(tf.expand_dims(depth_counter, 0), [max_length, 1])  # [max_length, depth]\n",
        "        depth_matrix = tf.pow(10000.0, tf.cast(depth_matrix / depth, fl_type))  # [max_length, depth]\n",
        "\n",
        "        # cos(x) == sin(x + π/2)\n",
        "        phase = tf.cast(tf.range(depth) % 2, fl_type) * math.pi / 2  # 0, π/2, 0, π/2, ...\n",
        "        phase_matrix = tf.tile(tf.expand_dims(phase, 0), [max_length, 1])  # [max_length, depth]\n",
        "\n",
        "        pos_counter = tf.range(max_length)\n",
        "        pos_matrix = tf.cast(tf.tile(tf.expand_dims(pos_counter, 1), [1, depth]), fl_type)  # [max_length, depth]\n",
        "\n",
        "        positional_encoding = tf.sin(pos_matrix / depth_matrix + phase_matrix)\n",
        "        # [batch_size, max_length, depth]\n",
        "        positional_encoding = tf.tile(tf.expand_dims(positional_encoding, 0), [batch_size, 1, 1])\n",
        "\n",
        "        return inputs + positional_encoding"
      ],
      "metadata": {
        "id": "f6TVExcZ_J2M"
      },
      "execution_count": 224,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenEmbedding(Layer):\n",
        "    def __init__(self, vocab_size, embedding_dim, embeddings=None, PAD_ID=0):\n",
        "        # vocab_size: 単語の総数\n",
        "        # embedding_dim: Embeddingの次数\n",
        "        super().__init__()\n",
        "        self.pad_id = PAD_ID\n",
        "        self.embedding_dim = embedding_dim\n",
        "        \n",
        "        self.embedding = Embedding(vocab_size, embedding_dim)\n",
        "        \n",
        "        if embeddings is None:\n",
        "            self.embedding = Embedding(input_dim=vocab_size,\n",
        "                                       output_dim=embedding_dim,\n",
        "                                       mask_zero=True,\n",
        "                                       trainable=True)\n",
        "        else:\n",
        "            self.embedding = Embedding(input_dim=embeddings.shape[0],\n",
        "                                       output_dim=embeddings.shape[1],\n",
        "                                       mask_zero=True,\n",
        "                                       trainable=True,\n",
        "                                       weights=[embeddings])\n",
        "\n",
        "    def call(self, x):\n",
        "        embedding = self.embedding(x)\n",
        "        return embedding * self.embedding_dim ** 0.5"
      ],
      "metadata": {
        "id": "vzyzasGm_Lyu"
      },
      "execution_count": 225,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(Layer):\n",
        "    \"\"\"\n",
        "    transformer block : before ->[attention -> FF]-> next\n",
        "    それぞれ残差接続とLayerNormalizationの処理が含まれる\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_dim, heads_num, drop_rate=0.1):\n",
        "        \"\"\"\n",
        "        hidden_numはheads_numで割り切れえる値とすること\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.atten = ResidualNormalizationWrapper(\n",
        "            layer = MultiHeadAttention(key_dim = 2, num_heads = heads_num, dropout = drop_rate),\n",
        "            drop_rate = drop_rate)\n",
        "        \n",
        "        self.ffn = ResidualNormalizationWrapper(\n",
        "            layer = FeedForwardNetwork(hidden_dim = hidden_dim, drop_rate = drop_rate),\n",
        "            drop_rate = drop_rate)\n",
        "    \n",
        "    def call(self, input, training, attention_mask=None, return_attention_scores=False):\n",
        "        \"\"\"\n",
        "        入力と出力で形式が変わらない\n",
        "        [batch_size, token_num, hidden_dim]\n",
        "        \"\"\"\n",
        "      \n",
        "        if return_attention_scores:\n",
        "            x, attn_weights = self.atten(x=input,value=input,return_attention_scores=return_attention_scores,attention_mask=None, training=training)\n",
        "            x = self.ffn(x)\n",
        "            return x, attn_weights\n",
        "        else:\n",
        "            x = self.atten(x=input,value=input, return_attention_scores=return_attention_scores,attention_mask=None, training=training)\n",
        "            x = self.ffn(x)\n",
        "            return x"
      ],
      "metadata": {
        "id": "4Q2ilzZt_Nqj"
      },
      "execution_count": 226,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(Layer):\n",
        "    '''\n",
        "    TransformerのEncoder\n",
        "    '''\n",
        "    def __init__(\n",
        "            self,\n",
        "            vocab_size, # 単語の総数\n",
        "            hopping_num, # Multi-head Attentionの繰り返し数\n",
        "            heads_num, # Multi-head Attentionのヘッド数\n",
        "            hidden_dim, # Embeddingの次数\n",
        "            token_num, # 系列長(文章中のトークン数)\n",
        "            drop_rate, # ドロップアウトの確率\n",
        "            embeddings=None\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.hopping_num = hopping_num\n",
        "        \n",
        "        # Embedding層\n",
        "        self.token_embedding = TokenEmbedding(vocab_size, hidden_dim, embeddings)\n",
        "        # Position Embedding\n",
        "        self.add_position_embedding = AddPositionalEncoding()\n",
        "        self.input_dropout_layer = Dropout(drop_rate)\n",
        "\n",
        "        # Multi-head Attentionの繰り返し(hopping)のリスト\n",
        "        self.attention_block_list = [TransformerBlock(hidden_dim, heads_num) for _ in range(hopping_num)]\n",
        "        self.output_normalization = LayerNormalization()\n",
        "\n",
        "    def call(\n",
        "            self,\n",
        "            input,\n",
        "            training,\n",
        "            attention_mask=None,\n",
        "            return_attention_scores=False\n",
        "    ):\n",
        "        '''\n",
        "        input: 入力 [batch_size, length]\n",
        "        memory: 入力 [batch_size, length]\n",
        "        attention_mask: attention weight に適用される mask\n",
        "            [batch_size, 1, q_length, k_length] \n",
        "            pad 等無視する部分が 0 となるようなもの(Decoderで使用)\n",
        "        return_attention_scores : attention weightを出力するか\n",
        "        出力 [batch_size, length, hidden_dim]\n",
        "        '''\n",
        "        # [batch_size, token_num] -> [batch_size, token_num, hidden_dim]\n",
        "        embedded_input = self.token_embedding(input)\n",
        "        # Positional Embedding\n",
        "        embedded_input = self.add_position_embedding(embedded_input)\n",
        "        query = self.input_dropout_layer(embedded_input, training=training)\n",
        "        \n",
        "        \n",
        "        if return_attention_scores:\n",
        "            # MultiHead Attentionを繰り返し適用\n",
        "            for i in range(self.hopping_num):\n",
        "                query, atten_weights = self.attention_block_list[i](query, training, attention_mask, return_attention_scores)\n",
        "\n",
        "            # [batch_size, token_num, hidden_dim]\n",
        "            return self.output_normalization(query), atten_weights\n",
        "        else:\n",
        "            # MultiHead Attentionを繰り返し適用\n",
        "            for i in range(self.hopping_num):\n",
        "                query = self.attention_block_list[i](query, training, attention_mask, return_attention_scores)\n",
        "\n",
        "            # [batch_size, token_num, hidden_dim]\n",
        "            return self.output_normalization(query)"
      ],
      "metadata": {
        "id": "KESDaLw5_Pga"
      },
      "execution_count": 227,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionClassifier(Model):\n",
        "    def __init__(\n",
        "            self,\n",
        "            vocab_size, # 単語の総数\n",
        "            hopping_num, # Multi-head Attentionの繰り返し数\n",
        "            heads_num, # Multi-head Attentionのヘッド数\n",
        "            hidden_dim, # Embeddingの次数\n",
        "            token_num, # 系列長(文章中のトークン数)\n",
        "            drop_rate, # ドロップアウトの確率\n",
        "            NUMLABELS, # クラス数\n",
        "            embeddings = None,\n",
        "            PAD_ID = 0\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.PAD_ID = PAD_ID\n",
        "        \n",
        "        self.encoder = Encoder(vocab_size, hopping_num, heads_num, hidden_dim, token_num, drop_rate, embeddings)\n",
        "        self.dense1 = Dense(hidden_dim, activation='tanh')\n",
        "        self.dropout1 = Dropout(drop_rate)   \n",
        "        self.final_layer = Dense(NUMLABELS, activation='softmax')\n",
        "\n",
        "    def call(self, x, training, return_attention_scores=False):\n",
        "        self_attention_mask=self._create_enc_attention_mask(x)\n",
        "        \n",
        "        # [batch_size, token_num] -> [batch_size, token_num, hidden_dim]\n",
        "        if return_attention_scores:\n",
        "            enc_output, atten_weights = self.encoder(input=x,training=training, attention_mask=self_attention_mask,return_attention_scores=return_attention_scores)\n",
        "        else:\n",
        "            enc_output = self.encoder(input=x,training=training, attention_mask=self_attention_mask,return_attention_scores=return_attention_scores)\n",
        "        \n",
        "        # 文頭の重みを使用 [batch_size, 0, hidden_dim]\n",
        "        # [batch_size, hidden_dim] -> [batch_size, hidden_dim]\n",
        "        enc_output = self.dense1(enc_output[:, 0, :])\n",
        "        enc_output = self.dropout1(enc_output)\n",
        "        \n",
        "        # [batch_size, hidden_dim] -> [batch_size, NUMLABELS]\n",
        "        final_output = self.final_layer(enc_output)\n",
        "\n",
        "        if return_attention_scores:\n",
        "            return final_output, atten_weights\n",
        "        else:\n",
        "            return final_output\n",
        "    \n",
        "    def _create_enc_attention_mask(self, x):\n",
        "        batch_size, length = tf.unstack(tf.shape(x))\n",
        "        # マスクする部分を1とする\n",
        "        pad_array = tf.cast(tf.equal(x, self.PAD_ID), tf.float32)  # [batch_size, token_num]\n",
        "        \n",
        "        # shape broadcasting で [batch_size, head_num, token_num, token_num] になる\n",
        "        return tf.reshape(pad_array, [batch_size, 1, 1, length])"
      ],
      "metadata": {
        "id": "1fmW5XMz_iRy"
      },
      "execution_count": 228,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install janome\n",
        "import re\n",
        "import pandas as pd\n",
        "from janome.tokenizer import Tokenizer\n",
        "j_t = Tokenizer(wakati=True)\n",
        "\n",
        "class Vocab(object):\n",
        "    def __init__(self):\n",
        "        self.w2i = {}\n",
        "        self.i2w = {}\n",
        "        self.special_chars = ['<pad>', '<s>', '</s>', '<unk>']\n",
        "        self.bos_char = self.special_chars[1]\n",
        "        self.eos_char = self.special_chars[2]\n",
        "        self.oov_char = self.special_chars[3]\n",
        "\n",
        "    def fit(self, sentences, path=None):\n",
        "        self._words = set()\n",
        "\n",
        "        #with open(path, 'r',encoding=\"utf-8\") as f:\n",
        "        #    sentences = f.read().splitlines()\n",
        "\n",
        "        for sentence in sentences:\n",
        "            #self._words.update(sentence.split())\n",
        "            self._words.update(sentence)\n",
        "\n",
        "        self.w2i = {w: (i + len(self.special_chars))\n",
        "                    for i, w in enumerate(self._words)}\n",
        "\n",
        "        for i, w in enumerate(self.special_chars):\n",
        "            self.w2i[w] = i\n",
        "\n",
        "        self.i2w = {i: w for w, i in self.w2i.items()}\n",
        "\n",
        "    def transform(self, sentences, path=None, bos=False, eos=False):\n",
        "        output = []\n",
        "\n",
        "        #with open(path, 'r',encoding=\"utf-8\") as f:\n",
        "        #    sentences = f.read().splitlines()\n",
        "\n",
        "        for sentence in sentences:\n",
        "            #sentence = sentence.split()\n",
        "            if bos:\n",
        "                sentence = [self.bos_char] + sentence\n",
        "            if eos:\n",
        "                sentence = sentence + [self.eos_char]\n",
        "            output.append(self.encode(sentence))\n",
        "\n",
        "        return output\n",
        "\n",
        "    def encode(self, sentence):\n",
        "        output = []\n",
        "\n",
        "        for w in sentence:\n",
        "            if w not in self.w2i:\n",
        "                idx = self.w2i[self.oov_char]\n",
        "            else:\n",
        "                idx = self.w2i[w]\n",
        "            output.append(idx)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def decode(self, sentence):\n",
        "        return [self.i2w[id] for id in sentence]\n",
        "\n",
        "\n",
        "def tokenizer_janome(text):\n",
        "    return [tok for tok in j_t.tokenize(text, wakati=True)]\n",
        "\n",
        "def preprocessing_text(text):\n",
        "    text = re.sub('\\r', '', text)\n",
        "    text = re.sub('\\n', '', text)\n",
        "    text = re.sub('　', '', text)\n",
        "    #text = re.sub(' ', '', text)\n",
        "    \n",
        "    text = re.sub(r'[0-9 ０-９]', '0', text)\n",
        "    return text\n",
        "\n",
        "def tokenizer_with_preprocessing(text):\n",
        "    text = preprocessing_text(text)\n",
        "    ret = tokenizer_janome(text)\n",
        "    return ret"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vbi1bK3v_nUk",
        "outputId": "94a9c2aa-f3f5-4f55-b505-19329fe7d391"
      },
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: janome in /usr/local/lib/python3.7/dist-packages (0.4.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "path='reviews.csv'\n",
        "\n",
        "df = pd.read_csv(path)\n",
        "seq_row = df['Body']\n",
        "y = df['Rating2']\n",
        "\n",
        "seq = [tokenizer_with_preprocessing(text) for text in seq_row]\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(seq, y, stratify=y)"
      ],
      "metadata": {
        "id": "1hac5NUD_qKd"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = Vocab()\n",
        "vocab.fit(x_train)\n",
        "\n",
        "x_id_train = vocab.transform(x_train, bos=True)\n",
        "x_id_test = vocab.transform(x_test, bos=True)"
      ],
      "metadata": {
        "id": "sKUltj4i_5VS"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "xnE9MIYO_9MU"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = pad_sequences(x_id_train, padding='post', maxlen=64)\n",
        "y_train_oht = tf.one_hot(y_train, depth=2, dtype=tf.float32)"
      ],
      "metadata": {
        "id": "qQ4y0xT-__iw"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AttentionClassifier(\n",
        "            vocab_size = len(vocab.i2w), # 単語の総数\n",
        "            hopping_num = 8, # Multi-head Attentionの繰り返し数\n",
        "            heads_num = 6, # Multi-head Attentionのヘッド数\n",
        "            hidden_dim = 300, # Embeddingの次数\n",
        "            drop_rate = 0.1, # ドロップアウトの確率\n",
        "            token_num = 64,\n",
        "            NUMLABELS = 2\n",
        "    )\n",
        "\n",
        "test = np.random.randint(0,2,(32,128))\n",
        "attention_mask = tf.constant(np.ones((32,1,128,128)), tf.float32)\n",
        "\n",
        "res = model(X_train[:10],False)"
      ],
      "metadata": {
        "id": "AMsQlBghABJc"
      },
      "execution_count": 229,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "import math\n",
        "\n",
        "criterion = tf.keras.losses.CategoricalCrossentropy()\n",
        "optimizer = optimizers.Adam(learning_rate=2e-4,\n",
        "                           beta_1=0.9, beta_2=0.999, amsgrad=True)\n",
        "\n",
        "def decay(epoch, steps=100):\n",
        "    initial_lrate = 2e-4\n",
        "    drop = 0.1\n",
        "    epochs_drop = 7\n",
        "    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
        "    return lrate\n",
        "\n",
        "lr_sc = LearningRateScheduler(decay, verbose=1)"
      ],
      "metadata": {
        "id": "k-ZhwUMJAC2s"
      },
      "execution_count": 230,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=criterion, optimizer=optimizer, metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "tUoQDtfIAE2q"
      },
      "execution_count": 231,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history=model.fit(X_train, y_train_oht, batch_size=32, epochs=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BUmE_rBAGhX",
        "outputId": "144e2831-0a4d-4dca-9bf9-958fe18323e2"
      },
      "execution_count": 232,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "119/119 [==============================] - 13s 46ms/step - loss: 0.6118 - accuracy: 0.7194\n",
            "Epoch 2/20\n",
            "119/119 [==============================] - 5s 45ms/step - loss: 0.5574 - accuracy: 0.7442\n",
            "Epoch 3/20\n",
            "119/119 [==============================] - 5s 45ms/step - loss: 0.3520 - accuracy: 0.8555\n",
            "Epoch 4/20\n",
            "119/119 [==============================] - 5s 45ms/step - loss: 0.1582 - accuracy: 0.9420\n",
            "Epoch 5/20\n",
            "119/119 [==============================] - 5s 46ms/step - loss: 0.0772 - accuracy: 0.9757\n",
            "Epoch 6/20\n",
            "119/119 [==============================] - 6s 48ms/step - loss: 0.0369 - accuracy: 0.9895\n",
            "Epoch 7/20\n",
            "119/119 [==============================] - 5s 45ms/step - loss: 0.0234 - accuracy: 0.9937\n",
            "Epoch 8/20\n",
            "119/119 [==============================] - 5s 45ms/step - loss: 0.0139 - accuracy: 0.9966\n",
            "Epoch 9/20\n",
            "119/119 [==============================] - 5s 45ms/step - loss: 0.0193 - accuracy: 0.9950\n",
            "Epoch 10/20\n",
            "119/119 [==============================] - 5s 45ms/step - loss: 0.0124 - accuracy: 0.9974\n",
            "Epoch 11/20\n",
            "119/119 [==============================] - 5s 45ms/step - loss: 0.0098 - accuracy: 0.9974\n",
            "Epoch 12/20\n",
            "119/119 [==============================] - 5s 45ms/step - loss: 0.0075 - accuracy: 0.9982\n",
            "Epoch 13/20\n",
            "119/119 [==============================] - 6s 47ms/step - loss: 0.0020 - accuracy: 0.9992\n",
            "Epoch 14/20\n",
            "119/119 [==============================] - 5s 46ms/step - loss: 0.0063 - accuracy: 0.9984\n",
            "Epoch 15/20\n",
            "119/119 [==============================] - 5s 45ms/step - loss: 0.0079 - accuracy: 0.9976\n",
            "Epoch 16/20\n",
            "119/119 [==============================] - 5s 45ms/step - loss: 0.0021 - accuracy: 0.9992\n",
            "Epoch 17/20\n",
            "119/119 [==============================] - 5s 45ms/step - loss: 0.0055 - accuracy: 0.9987\n",
            "Epoch 18/20\n",
            "119/119 [==============================] - 5s 45ms/step - loss: 0.0016 - accuracy: 0.9992\n",
            "Epoch 19/20\n",
            "119/119 [==============================] - 5s 45ms/step - loss: 0.0028 - accuracy: 0.9995\n",
            "Epoch 20/20\n",
            "119/119 [==============================] - 5s 45ms/step - loss: 1.9024e-04 - accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "layer = MultiHeadAttention(num_heads=2, key_dim=1, attention_axes=(2, 3))\n",
        "input_tensor = tf.keras.Input(shape=(10, 64, 300))\n",
        "output_tensor = layer(input_tensor, input_tensor)\n",
        "print(output_tensor.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJ1XrAndMIOF",
        "outputId": "35053daa-658a-45c2-da93-78ac37e06e67"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(None, 10, 64, 300)\n"
          ]
        }
      ]
    }
  ]
}