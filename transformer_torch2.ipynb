{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled10.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP80014Gw1jKsg5l5R8MSmb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tanakakao/test/blob/main/transformer_torch2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tK92N8ox8EPG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optimizers\n",
        "\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AddPositionalEncoding(nn.Module):\n",
        "    '''\n",
        "    入力テンソルに対し、位置の情報を付与して返すレイヤー\n",
        "    see: https://arxiv.org/pdf/1706.03762.pdf\n",
        "\n",
        "    PE_{pos, 2i}   = sin(pos / 10000^{2i / d_model})\n",
        "    PE_{pos, 2i+1} = cos(pos / 10000^{2i / d_model})\n",
        "    '''\n",
        "    def forward(self, inputs):\n",
        "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        fl_type = inputs.dtype\n",
        "        batch_size, max_length, depth = inputs.shape\n",
        "        \n",
        "        depth_counter = torch.div(torch.arange(depth) ,2, rounding_mode='trunc')*2\n",
        " \n",
        "        depth_matrix = torch.tile(torch.unsqueeze(depth_counter, 0), [max_length, 1])  # [max_length, depth]\n",
        "        depth_matrix = torch.pow(10000.0, depth_matrix / depth)  # [max_length, depth]\n",
        "        # cos(x) == sin(x + π/2)\n",
        "        phase = torch.remainder(torch.arange(depth), 2) * math.pi / 2\n",
        "        phase_matrix = torch.tile(torch.unsqueeze(phase, 0), [max_length, 1])  # [max_length, depth]\n",
        "\n",
        "        pos_counter = torch.arange(max_length)\n",
        "        pos_matrix = (torch.tile(torch.unsqueeze(pos_counter, 1), [1, depth]))  # [max_length, depth]\n",
        "\n",
        "        positional_encoding = torch.sin(pos_matrix / depth_matrix + phase_matrix)\n",
        "        # [batch_size, max_length, depth]\n",
        "        positional_encoding = torch.tile(torch.unsqueeze(positional_encoding, 0), [batch_size, 1, 1])\n",
        "        positional_encoding = positional_encoding.to(device)\n",
        "\n",
        "        return inputs + positional_encoding"
      ],
      "metadata": {
        "id": "IjvT-ssU8IpS"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, pretrained_weight=None):\n",
        "        # vocab_size: 単語の総数\n",
        "        # embedding_dim: Embeddingの次数\n",
        "        super().__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        \n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=1)\n",
        "        self.embedding.weight.requires_grad = True\n",
        "        \n",
        "        if pretrained_weight is not None:\n",
        "            self.embedding.weight.data.copy_(pretrained_weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # inputのIDに対応したベクトルを持ってくる\n",
        "        embedding = self.embedding(x)\n",
        "        \n",
        "        return embedding * (self.embedding_dim ** 0.5)"
      ],
      "metadata": {
        "id": "Y1uwz2DE8Mdg"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    '''\n",
        "    TransformerのEncoder\n",
        "    '''\n",
        "    def __init__(\n",
        "            self,\n",
        "            vocab_size, # 単語の総数\n",
        "            hopping_num, # Multi-head Attentionの繰り返し数\n",
        "            heads_num, # Multi-head Attentionのヘッド数\n",
        "            hidden_dim, # Embeddingの次数\n",
        "            token_num, # 系列長(文章中のトークン数)\n",
        "            drop_rate, # ドロップアウトの確率\n",
        "            pretrained_weight=None\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.hopping_num = hopping_num\n",
        "        \n",
        "        # Embedding層\n",
        "        self.token_embedding = TokenEmbedding(vocab_size, hidden_dim, pretrained_weight)\n",
        "        # Position Embedding\n",
        "        self.add_position_embedding = AddPositionalEncoding()\n",
        "        self.input_dropout_layer = nn.Dropout(drop_rate)\n",
        "\n",
        "        # Multi-head Attentionの繰り返し(hopping)のリスト\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=heads_num, dropout=drop_rate, batch_first=True)\n",
        "        self.attention_block = nn.TransformerEncoder(encoder_layer, num_layers=hopping_num)\n",
        "        \n",
        "        self.output_normalization = nn.LayerNorm(hidden_dim)\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            input,\n",
        "            memory=None,\n",
        "            attention_mask=None\n",
        "    ):\n",
        "        '''\n",
        "        input: 入力 [batch_size, length]\n",
        "        memory: 入力 [batch_size, length]\n",
        "        attention_mask: attention weight に適用される mask\n",
        "            [batch_size, 1, q_length, k_length] \n",
        "            pad 等無視する部分が 0 となるようなもの(Decoderで使用)\n",
        "        出力 [batch_size, length, hidden_dim]\n",
        "        '''\n",
        "        # [batch_size, token_num] -> [batch_size, token_num, hidden_dim]\n",
        "        embedded_input = self.token_embedding(input)\n",
        "        # Positional Embedding\n",
        "        embedded_input = self.add_position_embedding(embedded_input)\n",
        "        query = self.input_dropout_layer(embedded_input)\n",
        "        \n",
        "        query = self.attention_block(query, src_key_padding_mask=attention_mask)\n",
        "\n",
        "        # [batch_size, token_num, hidden_dim]\n",
        "        return self.output_normalization(query)"
      ],
      "metadata": {
        "id": "lrel8t_g8PTG"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionClassifier(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            vocab_size, # 単語の総数\n",
        "            hopping_num, # Multi-head Attentionの繰り返し数\n",
        "            heads_num, # Multi-head Attentionのヘッド数\n",
        "            hidden_dim, # Embeddingの次数\n",
        "            token_num, # 系列長(文章中のトークン数)\n",
        "            drop_rate, # ドロップアウトの確率\n",
        "            NUMLABELS, # クラス数\n",
        "            pretrained_weight=None,\n",
        "            PAD_ID = 1\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.PAD_ID = PAD_ID\n",
        "        \n",
        "        self.encoder = Encoder(vocab_size, hopping_num, heads_num, hidden_dim, token_num, drop_rate, pretrained_weight)\n",
        "        self.dense1 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.act1 = nn.Tanh()\n",
        "        self.dropout1 = nn.Dropout(drop_rate)   \n",
        "        self.final_layer = nn.Linear(hidden_dim, NUMLABELS)\n",
        "        \n",
        "        nn.init.normal_(self.dense1.weight, std=0.02)\n",
        "        nn.init.normal_(self.dense1.bias, std=0)\n",
        "        nn.init.normal_(self.final_layer.weight, std=0.02)\n",
        "        nn.init.normal_(self.final_layer.bias, std=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        self_attention_mask=self._create_enc_attention_mask(x)\n",
        "        \n",
        "        # [batch_size, token_num] -> [batch_size, token_num, hidden_dim]\n",
        "        enc_output = self.encoder(x, self_attention_mask)\n",
        "        \n",
        "        # 文頭の重みを使用 [batch_size, 0, hidden_dim]\n",
        "        # [batch_size, hidden_dim] -> [batch_size, hidden_dim]\n",
        "        enc_output = self.dense1(enc_output[:, 0, :])\n",
        "        enc_output = self.act1(enc_output)\n",
        "        enc_output = self.dropout1(enc_output)\n",
        "        \n",
        "        # [batch_size, hidden_dim] -> [batch_size, NUMLABELS]\n",
        "        final_output = self.final_layer(enc_output)\n",
        "\n",
        "        return final_output\n",
        "    \n",
        "    def _create_enc_attention_mask(self, x):\n",
        "        batch_size, length = x.shape\n",
        "        # マスクする部分を1とする\n",
        "        pad_array = torch.eq(x, self.PAD_ID).to(dtype=torch.int8)  # [batch_size, token_num]\n",
        "        \n",
        "        # shape broadcasting で [batch_size, head_num, token_num, token_num] になる\n",
        "        return pad_array.view([batch_size, length])"
      ],
      "metadata": {
        "id": "SWsNEbJR8XWv"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install janome\n",
        "import re\n",
        "from janome.tokenizer import Tokenizer\n",
        "j_t = Tokenizer(wakati=True)\n",
        "\n",
        "def tokenizer_janome(text):\n",
        "    return [tok for tok in j_t.tokenize(text, wakati=True)]\n",
        "\n",
        "def preprocessing_text(text):\n",
        "    text = re.sub('\\r', '', text)\n",
        "    text = re.sub('\\n', '', text)\n",
        "    text = re.sub('　', '', text)\n",
        "    text = re.sub(' ', '', text)\n",
        "    \n",
        "    text = re.sub(r'[0-9 ０-９]', '0', text)\n",
        "    return text\n",
        "\n",
        "def tokenizer_with_preprocessing(text):\n",
        "    text = preprocessing_text(text)\n",
        "    ret = tokenizer_janome(text)\n",
        "    return ret"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILxIm_PY8s9f",
        "outputId": "26dc2589-01cb-4e91-dbdc-8b31785df700"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting janome\n",
            "  Downloading Janome-0.4.2-py2.py3-none-any.whl (19.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.7 MB 1.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: janome\n",
            "Successfully installed janome-0.4.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchtext\n",
        "from torchtext import data, datasets\n",
        "from torchtext.legacy import data\n",
        "\n",
        "max_length = 64\n",
        "TEXT = data.Field(sequential=True, tokenize=tokenizer_with_preprocessing,\n",
        "                  use_vocab=True, lower=True, include_lengths=True,\n",
        "                  batch_first=True, fix_length=max_length,init_token=\"<eos>\",eos_token=\"<cls>\")\n",
        "LABEL = data.Field(sequential=False, use_vocab=False, preprocessing=None)\n",
        "\n",
        "dataset = data.TabularDataset(\n",
        "        path='reviews.csv', format='csv',\n",
        "        skip_header=True,\n",
        "        fields=[('Text', TEXT), ('Label', LABEL), ('Label2', LABEL)])\n",
        "\n",
        "train_dataset, test_dataset = dataset.split(split_ratio=0.7)\n",
        "train_dataset, val_dataset = train_dataset.split(split_ratio=0.7)\n",
        "\n",
        "train_iter = data.Iterator(\n",
        "    train_dataset, batch_size=32, \n",
        "    train=True  # train=Trueならシャッフルソートは有効\n",
        ")\n",
        "val_iter = data.Iterator(\n",
        "    val_dataset, batch_size=32, \n",
        "    train=False, sort=False\n",
        ")\n",
        "test_iter = data.Iterator(\n",
        "    test_dataset, batch_size=32, \n",
        "    train=False, sort=False\n",
        ")\n",
        "\n",
        "#train_iter, val_iter, test_iter = data.BucketIterator.splits((train_dataset, val_dataset, test_dataset), batch_size=32, repeat=False, shuffle=True)"
      ],
      "metadata": {
        "id": "T3ZPZGn88ZLa"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TEXT.build_vocab(train_dataset)\n",
        "vocab = TEXT.vocab\n",
        "len(vocab)\n",
        "\n",
        "batch = next(iter(train_iter))\n",
        "\n",
        "net = AttentionClassifier(\n",
        "            vocab_size = len(vocab), # 単語の総数\n",
        "            hopping_num = 8, # Multi-head Attentionの繰り返し数\n",
        "            heads_num = 6, # Multi-head Attentionのヘッド数\n",
        "            hidden_dim = 300, # Embeddingの次数\n",
        "            drop_rate = 0.1, # ドロップアウトの確率\n",
        "            token_num = 64,\n",
        "            \n",
        "    pretrained_weight=None,\n",
        "    NUMLABELS=2\n",
        "    )\n",
        "\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Linear') != -1:\n",
        "        nn.init.kaiming_normal_(m.weight)\n",
        "        if m.bias is not None:\n",
        "            nn.init.constant_(m.bias, 0.0)\n",
        "net.train()\n",
        "\n",
        "net.apply(weights_init)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lRPxQlNM8mB8",
        "outputId": "000e4b7c-d966-4652-bba6-fd8ef0863159"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AttentionClassifier(\n",
              "  (encoder): Encoder(\n",
              "    (token_embedding): TokenEmbedding(\n",
              "      (embedding): Embedding(10495, 300, padding_idx=1)\n",
              "    )\n",
              "    (add_position_embedding): AddPositionalEncoding()\n",
              "    (input_dropout_layer): Dropout(p=0.1, inplace=False)\n",
              "    (attention_block): TransformerEncoder(\n",
              "      (layers): ModuleList(\n",
              "        (0): TransformerEncoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=300, out_features=300, bias=True)\n",
              "          )\n",
              "          (linear1): Linear(in_features=300, out_features=2048, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (linear2): Linear(in_features=2048, out_features=300, bias=True)\n",
              "          (norm1): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm2): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout1): Dropout(p=0.1, inplace=False)\n",
              "          (dropout2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (1): TransformerEncoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=300, out_features=300, bias=True)\n",
              "          )\n",
              "          (linear1): Linear(in_features=300, out_features=2048, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (linear2): Linear(in_features=2048, out_features=300, bias=True)\n",
              "          (norm1): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm2): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout1): Dropout(p=0.1, inplace=False)\n",
              "          (dropout2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (2): TransformerEncoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=300, out_features=300, bias=True)\n",
              "          )\n",
              "          (linear1): Linear(in_features=300, out_features=2048, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (linear2): Linear(in_features=2048, out_features=300, bias=True)\n",
              "          (norm1): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm2): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout1): Dropout(p=0.1, inplace=False)\n",
              "          (dropout2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (3): TransformerEncoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=300, out_features=300, bias=True)\n",
              "          )\n",
              "          (linear1): Linear(in_features=300, out_features=2048, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (linear2): Linear(in_features=2048, out_features=300, bias=True)\n",
              "          (norm1): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm2): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout1): Dropout(p=0.1, inplace=False)\n",
              "          (dropout2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (4): TransformerEncoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=300, out_features=300, bias=True)\n",
              "          )\n",
              "          (linear1): Linear(in_features=300, out_features=2048, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (linear2): Linear(in_features=2048, out_features=300, bias=True)\n",
              "          (norm1): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm2): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout1): Dropout(p=0.1, inplace=False)\n",
              "          (dropout2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (5): TransformerEncoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=300, out_features=300, bias=True)\n",
              "          )\n",
              "          (linear1): Linear(in_features=300, out_features=2048, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (linear2): Linear(in_features=2048, out_features=300, bias=True)\n",
              "          (norm1): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm2): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout1): Dropout(p=0.1, inplace=False)\n",
              "          (dropout2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (6): TransformerEncoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=300, out_features=300, bias=True)\n",
              "          )\n",
              "          (linear1): Linear(in_features=300, out_features=2048, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (linear2): Linear(in_features=2048, out_features=300, bias=True)\n",
              "          (norm1): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm2): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout1): Dropout(p=0.1, inplace=False)\n",
              "          (dropout2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (7): TransformerEncoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=300, out_features=300, bias=True)\n",
              "          )\n",
              "          (linear1): Linear(in_features=300, out_features=2048, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (linear2): Linear(in_features=2048, out_features=300, bias=True)\n",
              "          (norm1): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm2): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout1): Dropout(p=0.1, inplace=False)\n",
              "          (dropout2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (output_normalization): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (dense1): Linear(in_features=300, out_features=300, bias=True)\n",
              "  (act1): Tanh()\n",
              "  (dropout1): Dropout(p=0.1, inplace=False)\n",
              "  (final_layer): Linear(in_features=300, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "learning_rate = 2e-5\n",
        "optimizer = optimizers.Adam(net.parameters(), lr=learning_rate, amsgrad=True, eps=1e-07)"
      ],
      "metadata": {
        "id": "xFwd0aXN80K2"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(net, dataloaders_dict, criterion, optimizer, num_epochs):\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"使用デバイス:\", device)\n",
        "    print('--------start--------')\n",
        "    net.to(device)\n",
        "    \n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                net.train()\n",
        "            else:\n",
        "                net.eval()\n",
        "            \n",
        "            epoch_loss = 0.0\n",
        "            epoch_corrects = 0\n",
        "            \n",
        "            for batch in (dataloaders_dict[phase]):\n",
        "                inputs = batch.Text[0].to(device)\n",
        "                labels = batch.Label2.to(device)\n",
        "                \n",
        "                optimizer.zero_grad()\n",
        "                \n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = net(inputs)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    \n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    \n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "                    \n",
        "                    epoch_loss += loss.item() * inputs.size(0)\n",
        "                    epoch_corrects += torch.sum(preds == labels.data)\n",
        "            \n",
        "            epoch_loss = epoch_loss / len(dataloaders_dict[phase].dataset)\n",
        "            epoch_acc = epoch_corrects.double() / len(dataloaders_dict[phase].dataset)\n",
        "            \n",
        "            print('Epoch {}/{} | {:.^5} | Loss: {:.4f} Acc: {:.4f}'.format(epoch+1,\n",
        "                                                                           num_epochs,\n",
        "                                                                           phase,\n",
        "                                                                           epoch_loss,\n",
        "                                                                           epoch_acc))\n",
        "        \n",
        "    return net"
      ],
      "metadata": {
        "id": "bpjXKFwI82a2"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloaders_dict = {\"train\":train_iter, \"val\":val_iter}"
      ],
      "metadata": {
        "id": "QxnmtYYY84Hc"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 20\n",
        "\n",
        "net_trained = train_model(net, dataloaders_dict, criterion, optimizer, num_epochs=num_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cUjnQnA86Wo",
        "outputId": "5244e893-2a79-4c6d-9ba0-bec36e236371"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "使用デバイス: cuda:0\n",
            "--------start--------\n",
            "Epoch 1/20 | train | Loss: 0.5990 Acc: 0.7360\n",
            "Epoch 1/20 | .val. | Loss: 0.5695 Acc: 0.7467\n",
            "Epoch 2/20 | train | Loss: 0.6028 Acc: 0.7307\n",
            "Epoch 2/20 | .val. | Loss: 0.5654 Acc: 0.7467\n",
            "Epoch 3/20 | train | Loss: 0.5920 Acc: 0.7319\n",
            "Epoch 3/20 | .val. | Loss: 0.5258 Acc: 0.7467\n",
            "Epoch 4/20 | train | Loss: 0.5546 Acc: 0.7235\n",
            "Epoch 4/20 | .val. | Loss: 0.4851 Acc: 0.7702\n",
            "Epoch 5/20 | train | Loss: 0.5071 Acc: 0.7646\n",
            "Epoch 5/20 | .val. | Loss: 0.5145 Acc: 0.7411\n",
            "Epoch 6/20 | train | Loss: 0.4628 Acc: 0.7957\n",
            "Epoch 6/20 | .val. | Loss: 0.4772 Acc: 0.8107\n",
            "Epoch 7/20 | train | Loss: 0.4444 Acc: 0.7994\n",
            "Epoch 7/20 | .val. | Loss: 0.4284 Acc: 0.8126\n",
            "Epoch 8/20 | train | Loss: 0.4025 Acc: 0.8256\n",
            "Epoch 8/20 | .val. | Loss: 0.5038 Acc: 0.8013\n",
            "Epoch 9/20 | train | Loss: 0.3900 Acc: 0.8321\n",
            "Epoch 9/20 | .val. | Loss: 0.4560 Acc: 0.8277\n",
            "Epoch 10/20 | train | Loss: 0.3778 Acc: 0.8450\n",
            "Epoch 10/20 | .val. | Loss: 0.4860 Acc: 0.7957\n",
            "Epoch 11/20 | train | Loss: 0.3711 Acc: 0.8446\n",
            "Epoch 11/20 | .val. | Loss: 0.4467 Acc: 0.8418\n",
            "Epoch 12/20 | train | Loss: 0.3409 Acc: 0.8555\n",
            "Epoch 12/20 | .val. | Loss: 0.4081 Acc: 0.8333\n",
            "Epoch 13/20 | train | Loss: 0.3348 Acc: 0.8652\n",
            "Epoch 13/20 | .val. | Loss: 0.4520 Acc: 0.8390\n",
            "Epoch 14/20 | train | Loss: 0.3304 Acc: 0.8672\n",
            "Epoch 14/20 | .val. | Loss: 0.4708 Acc: 0.8183\n",
            "Epoch 15/20 | train | Loss: 0.3323 Acc: 0.8591\n",
            "Epoch 15/20 | .val. | Loss: 0.6139 Acc: 0.7881\n",
            "Epoch 16/20 | train | Loss: 0.3022 Acc: 0.8740\n",
            "Epoch 16/20 | .val. | Loss: 0.4426 Acc: 0.8427\n",
            "Epoch 17/20 | train | Loss: 0.3009 Acc: 0.8740\n",
            "Epoch 17/20 | .val. | Loss: 0.4333 Acc: 0.8437\n",
            "Epoch 18/20 | train | Loss: 0.2723 Acc: 0.8938\n",
            "Epoch 18/20 | .val. | Loss: 0.5165 Acc: 0.8183\n",
            "Epoch 19/20 | train | Loss: 0.2850 Acc: 0.8805\n",
            "Epoch 19/20 | .val. | Loss: 0.4997 Acc: 0.8079\n",
            "Epoch 20/20 | train | Loss: 0.2971 Acc: 0.8720\n",
            "Epoch 20/20 | .val. | Loss: 0.6061 Acc: 0.7702\n"
          ]
        }
      ]
    }
  ]
}